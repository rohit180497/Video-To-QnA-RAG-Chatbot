{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c98d39c0",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbd6ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "import ast\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddaaa10",
   "metadata": {},
   "source": [
    "### Functions for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d7415b10-0896-476e-9a5f-4b6b8b75ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Example of a free text corpus\n",
    "# corpus_text = \"\"\"\n",
    "# Audi presens can help prepare for and in some cases help prevent collisions.Audi's progress is amplifying your instincts. \n",
    "# Get an exceptional offer at your local Audi dealer. As a man speeds down a country road in his Audi Q7, its pre-sense safety technology responds to a deer leaping suddenly in front of his car. \n",
    "# Without missing a beat, the vehicle skids to a stop before the hair on the startled man's arms has time to rise. The Audi pre-sense is now available in the Audi A4.\n",
    "# \"\"\"\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=80):\n",
    "    \"\"\"\n",
    "    Splits the input text into smaller chunks of specified size.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be split into chunks.\n",
    "    chunk_size (int, optional): The size of each chunk. Defaults to 80.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the text chunks.\n",
    "    \n",
    "    This function takes a string `text` and splits it into chunks of size `chunk_size`.\n",
    "    It iterates through the text, appending each chunk of the specified size to the \n",
    "    `chunks` list. The result is a list of text chunks, where each chunk is at most \n",
    "    `chunk_size` characters long.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Generates embeddings for each chunk of text using a Sentence-BERT model.\n",
    "\n",
    "    Args:\n",
    "    chunks (list): List of strings where each string represents a chunk of text.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: 2D array of embeddings where each row corresponds to the embedding of a chunk.\n",
    "\n",
    "    This function initializes a Sentence-BERT model ('paraphrase-MiniLM-L6-v2')\n",
    "    and computes embeddings for each chunk of text in the input list `chunks`. The embeddings\n",
    "    are computed using the Sentence-BERT model and stored in a list. These embeddings are then\n",
    "    stacked vertically to form a 2D numpy array. Each row in the array represents the embedding\n",
    "    vector of a chunk of text.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        chunk_embedding = model.encode(chunk)\n",
    "        embeddings.append(chunk_embedding)\n",
    "    \n",
    "    # Stack embeddings into a numpy array\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def build_faiss_index(embeddings, dimension=384):\n",
    "    \"\"\"\n",
    "    Builds a FAISS index for fast similarity search using the given embeddings.\n",
    "\n",
    "    Args:\n",
    "    embeddings (numpy.ndarray): 2D array of embeddings where each row represents the embedding vector of a chunk of text.\n",
    "    dimension (int, optional): Dimensionality of the embedding vectors. Default is 384.\n",
    "\n",
    "    Returns:\n",
    "    faiss.IndexFlatL2: FAISS index object configured with L2 distance metric, populated with the provided embeddings.\n",
    "\n",
    "    This function initializes a FAISS index with the specified dimensionality and adds the given embeddings to it.\n",
    "    The FAISS index is optimized for efficient similarity search based on L2 distance metric. It returns the initialized\n",
    "    FAISS index object ready for use in similarity searches.\n",
    "    \"\"\"\n",
    "    # Build FAISS index\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance metric\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def retrieve_documents(user_question, index, chunks):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents (chunks of text) based on the user question using a pre-built FAISS index.\n",
    "\n",
    "    Args:\n",
    "    user_question (str): User query for retrieving relevant documents.\n",
    "    index (faiss.IndexFlatL2): Pre-built FAISS index object for similarity search.\n",
    "    chunks (list): List of text chunks corresponding to the embeddings used for indexing.\n",
    "\n",
    "    Returns:\n",
    "    list: List of relevant text chunks retrieved based on the similarity search using FAISS.\n",
    "\n",
    "    This function encodes the user question using a Sentence-BERT model, then performs a vector search using\n",
    "    FAISS to retrieve the closest embeddings to the query. It returns the corresponding text chunks that are\n",
    "    most relevant to the user question based on the similarity scores obtained from FAISS.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate embedding for user question (simulated)\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode(user_question)\n",
    "    # print('Query embedding:', query_embedding)\n",
    "    # Perform vector search using FAISS\n",
    "    query_embedding = query_embedding.astype(np.float32).reshape(1, -1)  # Convert to 2D array \n",
    "    # print('Query embedding:', query_embedding)  \n",
    "\n",
    "    # print(f\"Query embedding shape: {query_embedding.shape}\")\n",
    "    # print(f\"Expected dimension (d): {index.d}\")\n",
    "\n",
    "    D, I = index.search(query_embedding, 5)  # Retrieving top 5 closest embeddings\n",
    "    \n",
    "    # Retrieve relevant chunks based on FAISS results\n",
    "    relevant_docs = [chunks[i] for i in I.flatten()]\n",
    "    # print('relevant docs:', relevant_docs)\n",
    "    return relevant_docs\n",
    "\n",
    "def semantic_search(user_question, relevant_docs):\n",
    "    \"\"\"\n",
    "    Performs semantic search to determine if there are relevant documents (chunks of text) that match the user question.\n",
    "\n",
    "    Args:\n",
    "    user_question (str): User query to find relevant documents.\n",
    "    relevant_docs (list): List of relevant text chunks retrieved from the FAISS index.\n",
    "\n",
    "    Returns:\n",
    "    str: \"Yes\" if there are relevant documents with cosine similarity above a threshold, otherwise \"No\".\n",
    "\n",
    "    This function encodes the user question and relevant documents using a Sentence-BERT model,\n",
    "    calculates cosine similarity scores, and determines if any relevant documents match the user question\n",
    "    based on a predefined similarity threshold. It returns \"Yes\" if a relevant match is found, otherwise \"No\".\n",
    "    \"\"\"\n",
    "    # Initialize Sentence-BERT model (example model)\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    \n",
    "    # Encode user question\n",
    "    query_embedding = model.encode(user_question, convert_to_tensor=True)\n",
    "    # Encode relevant documents\n",
    "    doc_embeddings = model.encode(relevant_docs, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate cosine similarity between user question and relevant documents\n",
    "    cos_sim_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)\n",
    "    # print(cos_sim_scores)\n",
    "    # Select documents with highest similarity scores\n",
    "    threshold = 0.25  # Adjust as needed\n",
    "    # filtered_docs = [relevant_docs[i] for i in range(len(relevant_docs)) if cos_sim_scores[0][i] > threshold]\n",
    "    if cos_sim_scores.max() > 0.25:  # You can adjust this threshold as needed\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "30c17faa-2c47-4afd-973f-f3e9b786a4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/long_form_data1.xlsx')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44e6bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Labels'] = df['Labels'].apply(lambda x: ast.literal_eval(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "12c758d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Predefined set of questions\n",
    "questions = [\n",
    "    \"Is there a call to go online (e.g., shop online, visit the Web)?\",\n",
    "    \"Is there online contact information provided (e.g., URL, website)?\",\n",
    "    \"Is there a visual or verbal call to purchase (e.g., buy now, order now)?\",\n",
    "    \"Does the ad portray a sense of urgency to act (e.g., buy before sales ends, order before ends)?\",\n",
    "    \"Is there an incentive to buy (e.g., a discount, a coupon, a sale or 'limited time offer')?\",\n",
    "    \"Is there offline contact information provided (e.g., phone, mail, store location)?\",\n",
    "    \"Is there mention of something free?\",\n",
    "    \"Does the ad mention at least one specific product or service (e.g., model, type, item)?\",\n",
    "    \"Is there any verbal or visual mention of the price?\",\n",
    "    \"Does the ad show the brand (logo, brand name) or trademark (something that most people know is the brand) multiple times?\",\n",
    "    \"Does the ad show the brand or trademark exactly once at the end of the ad?\",\n",
    "    \"Is the ad intended to affect the viewer emotionally, either with positive emotion or negative emotion?\",\n",
    "    \"Does the ad give you a positive feeling about the brand?\",\n",
    "    \"Does the ad have a story arc, with a beginning and an end?\",\n",
    "    \"Does the ad have a reversal of fortune, where something changes for the better or worse?\",\n",
    "    \"Does the ad have relatable characters?\",\n",
    "    \"Is the ad creative/clever?\",\n",
    "    \"Is the ad intended to be funny?\",\n",
    "    \"Does this ad provide sensory stimulation?\",\n",
    "    \"Is the ad visually pleasing?\",\n",
    "    \"Does the ad have cute elements like animals, babies, animated characters, etc?\"\n",
    "]\n",
    "\n",
    "question_embeddings = []\n",
    "for question in questions:\n",
    "    encoded_input = tokenizer(question, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "        embeddings = output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    question_embeddings.append(embeddings)\n",
    "#( Need to remove embeddings from the list to save memory as not using this embeddings)\n",
    "# Convert to numpy array for consistency\n",
    "# query_embedding = np.array(question_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13cd43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single corpus\n",
    "def process_corpus(corpus, questions):\n",
    "    \"\"\"\n",
    "    Processes a corpus of text, performs semantic search for each question, and returns answers.\n",
    "\n",
    "    Args:\n",
    "    corpus (str): Full text corpus to process.\n",
    "    question_embeddings (list): List of embeddings for each question.\n",
    "    questions (list): List of questions to answer based on the corpus.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are questions and values are answers (\"Yes\" or \"No\").\n",
    "\n",
    "    This function splits the corpus into text chunks, generates embeddings for each chunk,\n",
    "    builds a FAISS index for efficient retrieval, and performs semantic search to answer\n",
    "    each question based on the relevant chunks of text. It returns a dictionary of questions\n",
    "    mapped to their corresponding answers.\n",
    "    \"\"\"\n",
    "    chunks = split_text_into_chunks(corpus)\n",
    "    embeddings = generate_embeddings(chunks)\n",
    "    index = build_faiss_index(embeddings)\n",
    "    results = {}\n",
    "\n",
    "    for question in questions:\n",
    "        relevant_docs = retrieve_documents(question, index, chunks)\n",
    "        answer = semantic_search(question, relevant_docs)\n",
    "        results[question] = answer\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a05a8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embedding.reshape(1, -1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "119e6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store evaluation results\n",
    "speech_list = []\n",
    "predicted_labels_list = []\n",
    "recall_list = []\n",
    "precision_list = []\n",
    "f1_score_list = []\n",
    "agreement_percentage_list = []\n",
    "accuracy_list = []\n",
    "roc_auc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1ff3421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation Done:  0\n",
      "Operation Done:  1\n",
      "Operation Done:  2\n",
      "Operation Done:  3\n",
      "Operation Done:  4\n",
      "Operation Done:  5\n",
      "Operation Done:  6\n",
      "Operation Done:  7\n",
      "Operation Done:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\on-campus\\data_initiative\\task2\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation Done:  9\n",
      "Operation Done:  10\n",
      "Operation Done:  11\n",
      "Operation Done:  12\n",
      "Operation Done:  13\n",
      "Operation Done:  14\n",
      "Operation Done:  15\n",
      "Operation Done:  16\n",
      "Operation Done:  17\n",
      "Operation Done:  18\n",
      "Operation Done:  19\n",
      "Operation Done:  20\n",
      "Operation Done:  21\n",
      "Operation Done:  22\n",
      "Operation Done:  23\n",
      "Operation Done:  24\n",
      "Operation Done:  25\n",
      "Operation Done:  26\n",
      "Operation Done:  27\n",
      "Operation Done:  28\n",
      "Operation Done:  29\n",
      "Operation Done:  30\n",
      "Operation Done:  31\n",
      "Operation Done:  32\n",
      "Operation Done:  33\n",
      "Operation Done:  34\n",
      "Operation Done:  35\n",
      "Operation Done:  36\n",
      "Operation Done:  37\n",
      "Operation Done:  38\n",
      "Operation Done:  39\n",
      "Operation Done:  40\n",
      "Operation Done:  41\n",
      "Operation Done:  42\n",
      "Operation Done:  43\n",
      "Operation Done:  44\n",
      "Operation Done:  45\n",
      "Operation Done:  46\n",
      "Operation Done:  47\n",
      "Operation Done:  48\n",
      "Operation Done:  49\n",
      "Operation Done:  50\n",
      "Operation Done:  51\n",
      "Operation Done:  52\n",
      "Operation Done:  53\n",
      "Operation Done:  54\n",
      "Operation Done:  55\n",
      "Operation Done:  56\n",
      "Operation Done:  57\n",
      "Operation Done:  58\n",
      "Operation Done:  59\n",
      "Operation Done:  60\n",
      "Operation Done:  61\n",
      "Operation Done:  62\n",
      "Operation Done:  63\n",
      "Operation Done:  64\n",
      "Operation Done:  65\n",
      "Operation Done:  66\n",
      "Operation Done:  67\n",
      "Operation Done:  68\n",
      "Operation Done:  69\n",
      "Operation Done:  70\n",
      "Operation Done:  71\n",
      "Operation Done:  72\n",
      "Operation Done:  73\n",
      "Operation Done:  74\n",
      "Operation Done:  75\n",
      "Operation Done:  76\n",
      "Operation Done:  77\n",
      "Operation Done:  78\n",
      "Operation Done:  79\n",
      "Operation Done:  80\n",
      "Operation Done:  81\n",
      "Operation Done:  82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\on-campus\\data_initiative\\task2\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation Done:  83\n",
      "Operation Done:  84\n",
      "Operation Done:  85\n",
      "Operation Done:  86\n",
      "Operation Done:  87\n",
      "Operation Done:  88\n",
      "Operation Done:  89\n",
      "Operation Done:  90\n",
      "Operation Done:  91\n",
      "Operation Done:  92\n",
      "Operation Done:  93\n",
      "Operation Done:  94\n",
      "Operation Done:  95\n",
      "Operation Done:  96\n",
      "Operation Done:  97\n",
      "Operation Done:  98\n",
      "Operation Done:  99\n",
      "Operation Done:  100\n",
      "Operation Done:  101\n",
      "Operation Done:  102\n",
      "Operation Done:  103\n",
      "Operation Done:  104\n",
      "Operation Done:  105\n",
      "Operation Done:  106\n",
      "Operation Done:  107\n",
      "Operation Done:  108\n",
      "Operation Done:  109\n",
      "Operation Done:  110\n",
      "Operation Done:  111\n",
      "Operation Done:  112\n",
      "Operation Done:  113\n",
      "Operation Done:  114\n",
      "Operation Done:  115\n",
      "Operation Done:  116\n",
      "Operation Done:  117\n",
      "Operation Done:  118\n",
      "Operation Done:  119\n",
      "Operation Done:  120\n",
      "Operation Done:  121\n",
      "Operation Done:  122\n",
      "Operation Done:  123\n",
      "Operation Done:  124\n",
      "Operation Done:  125\n",
      "Operation Done:  126\n",
      "Operation Done:  127\n",
      "Operation Done:  128\n",
      "Operation Done:  129\n",
      "Operation Done:  130\n",
      "Operation Done:  131\n",
      "Operation Done:  132\n",
      "Operation Done:  133\n",
      "Operation Done:  134\n",
      "Operation Done:  135\n",
      "Operation Done:  136\n",
      "Operation Done:  137\n",
      "Operation Done:  138\n",
      "Operation Done:  139\n",
      "Operation Done:  140\n",
      "Operation Done:  141\n",
      "Operation Done:  142\n",
      "Operation Done:  143\n",
      "Operation Done:  144\n",
      "Operation Done:  145\n",
      "Operation Done:  146\n",
      "Operation Done:  147\n",
      "Operation Done:  148\n",
      "Operation Done:  149\n"
     ]
    }
   ],
   "source": [
    "# Initializing variable to accumulate total agreements\n",
    "total_agreement = 0\n",
    "\n",
    "# Iterating over each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    user_input_text = row['Speech'] + row['description']\n",
    "    labels = row['Labels']\n",
    "\n",
    "    # Process the corpus to get answers for all questions\n",
    "    answers = process_corpus(user_input_text, questions)\n",
    "    print(\"Operation Done: \", index)\n",
    "    # Convert answers to binary format for evaluation\n",
    "    predicted_answers = [1 if answers[question] == \"Yes\" else 0 for question in questions]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    recall = recall_score(labels, predicted_answers, average='binary')\n",
    "    precision = precision_score(labels, predicted_answers, average='binary')\n",
    "    f1 = f1_score(labels, predicted_answers, average='binary')\n",
    "    accuracy = accuracy_score(labels, predicted_answers)\n",
    "    roc_auc = roc_auc_score(labels, predicted_answers)\n",
    "\n",
    "    # Append results to lists\n",
    "    speech_list.append(user_input_text)\n",
    "    predicted_labels_list.append(predicted_answers)\n",
    "    recall_list.append(recall)\n",
    "    precision_list.append(precision)\n",
    "    f1_score_list.append(f1)\n",
    "    accuracy_list.append(accuracy)\n",
    "    roc_auc_list.append(roc_auc)\n",
    "\n",
    "    # Calculate agreement percentage\n",
    "    agreement_count = sum([1 for true, pred in zip(labels, predicted_answers) if true == pred])\n",
    "    total_agreement += agreement_count\n",
    "    agreement_percentage = (agreement_count / len(labels)) * 100\n",
    "    agreement_percentage_list.append(agreement_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4eb22896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with results\n",
    "results_df = pd.DataFrame({\n",
    "    'Speech': speech_list,\n",
    "    'Predicted Labels': predicted_labels_list,\n",
    "    'Recall': recall_list,\n",
    "    'Precision': precision_list,\n",
    "    'F1 Score': f1_score_list,\n",
    "    'Accuracy': accuracy_list,\n",
    "    'ROC AUC': roc_auc_list,\n",
    "    'Agreement Percentage': agreement_percentage_list\n",
    "\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1d475366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech</th>\n",
       "      <th>Predicted Labels</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Agreement Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's another pure gray morning. Don't know wha...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>47.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The end of civilization is upon us. Hold your ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.533654</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audi presens can help prepare for and in some ...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, ...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>61.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The new Honda Odyssey has tons of available sm...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>52.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi guys. So this is the all new Chevy Equinox....</td>\n",
       "      <td>[0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.422727</td>\n",
       "      <td>42.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Speech  \\\n",
       "0  It's another pure gray morning. Don't know wha...   \n",
       "1  The end of civilization is upon us. Hold your ...   \n",
       "2  Audi presens can help prepare for and in some ...   \n",
       "3  The new Honda Odyssey has tons of available sm...   \n",
       "4  Hi guys. So this is the all new Chevy Equinox....   \n",
       "\n",
       "                                    Predicted Labels    Recall  Precision  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  0.000000   0.000000   \n",
       "1  [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, ...  0.375000   0.428571   \n",
       "2  [0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, ...  0.600000   0.600000   \n",
       "3  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...  0.272727   0.600000   \n",
       "4  [0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, ...  0.300000   0.375000   \n",
       "\n",
       "   F1 Score  Accuracy   ROC AUC  Agreement Percentage  \n",
       "0  0.000000  0.476190  0.454545             47.619048  \n",
       "1  0.400000  0.571429  0.533654             57.142857  \n",
       "2  0.600000  0.619048  0.618182             61.904762  \n",
       "3  0.375000  0.523810  0.536364             52.380952  \n",
       "4  0.333333  0.428571  0.422727             42.857143  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4a19ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ad40b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame(results_df['Predicted Labels'].tolist(), columns=questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "69436d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 29)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.concat([results_df, label_df], axis=1)\n",
    "score_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "211c8b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_excel('results/RAG_approach.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bd3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
